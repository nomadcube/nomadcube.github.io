---
layout: post
tag: 机器学习
---

### **模型原理**
$$K$$个类别的判别问题，只需对比$$P(c_k|x), k = 1, 2, ..., K $$之间的大小就可以得到特定$$x$$的预测类别。因此判别问题的关键在于求条件概率$$P(c_k|x)$$.
具体到贝叶斯模型中，条件概率$$P(c_k|x)$$可以看作是类别的后验概率，可由贝叶斯公式推导而来，即$$P(c_k|x) = \frac{P(x|c_k)P(c_k)}{P(x)} \propto P(x|c_k)P(c_k)$$，相当于用似然值$$P(x|c_k)$$去更新类别的先验概率$$P(c_k)$$，得到后验概率。
因此对贝叶斯模型做参数估计时，先验概率$$P(c_k)$$可以用数据中的类别出现频率来估计，而计算$$P(x|c_k)$$则需要先对x的条件分布作一些假设，以缩小解空间，而且不同的分布假设对应不同的朴素贝叶斯模型变式。比如多项式朴素贝叶斯，它的对$$x$$条件分布的假设是多项式分布，即给定任意一个分类$$c_k$$的前提下，$$x$$服从多项式分布 $$m(p_{k,1}, p_{k,2}, ..., p_{k,M})$$.因此对于有$$K$$个类别的判别问题，需要对$$M*K$$个参数进行估计，其中$$M$$为特征数目。

### **参数估计**
首先，对于多项式分布，参数的极大似然估计是无偏估计，因此对于任意一个多项式分布$$m(p_{1}, p_{2}, ..., p_{M})$$，参数$$p_{i}$$的估计值为$$\frac{N_i}{N}$$, 其中$$N$$为总试验次数，$$N_i$$为第$$i$$个特征出现的试验次数。
对于多项式朴素贝叶斯模型而言，道理也是类似的。正如上文所说，$$K$$个类别的判别问题对应有$$K$$个多项式分布，每一个多项式分布的参数都可以用$$\frac{N_{k,i}}{N_{k}}$$这样的形式来估计。

### **文本分类的例子**
多项式朴素贝叶斯常见于文本分类中，因此以文本分类为例，将多项式朴素贝叶斯模型梳理一遍。

#### **1. 样本数据**

假设一个语料中共有$$N$$条文档作为样本，可以区分到$$K$$个类别$$c_1, c_2, ..., c_K$$中，并有$$M$$个词条作为特征。记样本集为$$X = (x^{(1)}, x^{(2)}, ..., x^{(N)})$$, $$Y = (c^{(1)}, c^{(2)}..., c^{(N)})$$.
这时候模型的假设是任意给定类别$$c_k$$下的任意一个文档$$x$$的概率密度函数为$$P(x|c_k) = \binom{x_1}{N}\binom{x_2}{N-x_1}...\binom{x_M}{N-x_1-...-x_{M-1}} {w_{k,1}}^{x_{1}}...{w_{k,M}}^{x_{M}}$$，因此后验概率$$P(c_k|x) \propto P(x|c_k)P(c_k) \propto {w_{k,1}}^{x_{1}}...{w_{k,M}}^{x_{M}}P(c_k) $$.

#### **2. 参数估计**
从中可以推断出两组参数：

- 各类别的先验概率

$$b = (b_1, b_2, ..., b_K)$$, 其中$$b_k = \frac{\sum_{i = 1}^{N} I(c^{(i)} == c_k)}{N}$$，相当于各类别对应的文档个数占总文档个数的比例。

- 各类别对应的多项式分布的参数

$$w = [(w_{1,1}, ..., w_{1,M}), ..., (w_{K,1}, ..., w_{K,M})]$$, 其中$$w_{k,i} = \frac{N_{k,i}}{N_{k}}$$, 相当于类别$$k$$的所有文档中词条$$i$$的频次占类别$$k$$的所有文档总词条频次的比例。

#### **3. 分类预测**
得到模型参数之后就可以进行预测。假设待预测样本$$x = (x_1, x_2, ..., x_M)$$, $$x_i$$相当于词条$$i$$在这个样本中的出现次数。由于类别$$c_k$$的后验概率估计值为$$P(c_k|x) \propto {w_{k,1}}^{x_{1}}...{w_{k,M}}^{x_{M}}b_k$$，由$$x, b, w$$可以计算出样本$$x$$下各个类别的后验概率，从而选出后验概率最高的类别作为预测结果。

### **程序实现**

实现分类算法时最重要的三个模块：训练、预测、模型评估，其中模型评估是指用已有的模型对一组测试样本进行预测，将预测结果和真实类别进行比较计算出评价指标，其中计算评价指标与具体模型无关，可作为独立于模型以外的公共模块，因此在这里先忽略这个模块。

#### **1. 训练**
训练可以认为是不同的分类模型都不同，可抽象出来的东西不多。当然对于判别算法来说就不一样，因为它们之间可以共享用最优化方法求解或正则化等功能。但多项式朴素贝叶斯是生成模型，不需要最优化求解，相反，它的求解特别简单，可以认为关键步骤在于进行“计数”，分别计算类别的出现频率、类别和特征的同现频次，如以下代码段：

{% highlight python %}

def fit(y, x, alpha=0.):

    num_class = y.max() + 1
    num_sample, num_feature = x.shape
    class_mat = np.zeros((num_class, num_sample))
    for i in xrange(num_sample):
        class_mat[y[i]][i] = 1.
    b = class_mat.sum(axis=1) / class_mat.sum()

    class_feature_co_cnt = np.dot(class_mat, x) + alpha
    class_cnt = np.array(class_feature_co_cnt.sum(axis=1).ravel())[0]
    w = class_feature_co_cnt.transpose() / class_cnt

    return b, np.log(w.transpose())
{% endhighlight %}

其中```b = class_mat.sum(axis=1) / class_mat.sum()```是计算类别的出现频率，即估计类别的先验概率，```class_feature_co_cnt = np.dot(class_mat, x) + alpha```是计算类别和特征的同现频次并用```alpha```作为平滑系数，```w = class_feature_co_cnt.transpose() / class_cnt```则完成了所有多项式分布的参数估计。

#### **2. 预测**
至于预测，它对于线性模型来说是相似的，都是将参数矩阵和样本矩阵进行相乘，得到条件概率/后验概率矩阵，再按行或列取最大值。这里想说的重点是，可以对多项式朴素贝叶斯模型后验概率的公式两边取对数，变换成线性模型的形式，而取对数是递增函数，因此：$$logP(c_k|x) \propto {x_{1}}log{w_{k,1}} + ... + {x_{M}}log{w_{k,M}} + logP(c_k) $$, 相当于将参数$$b, w$$组合成线性分类器的超平面，同样是对比$$K$$个$$logP(c_k|x)$$得到分类预测结果。对参数作变换之后的预测模型就变得很简洁：

{% highlight python %}
def predict(b, w, new_x):
    prob_mat = new_x.dot(w.transpose()) + b
    return np.argmax(prob_mat, axis=1)
{% endhighlight %}
