---
layout: post
tags: 层次模型 最优化
---

在同时包含观测变量X和隐变量Z的场景中，若隐变量是离散的，那么由X和Z的联合概率密度$$P(X,Z)$$可以得到X的边缘分布$$P(X) = \sum_k P(X;Z_k)P(Z_k)$$. 这也称为层次模型，层次模型所解决的场景主要有这些特点：

1. Z变量会影响X变量。假如这X和Z独立，那么也没必要用$$P(X;Z)$$了，因为X和Z独立的前提下，$$P(X;Z) = P(X)$$
2. Z变量分布已知，但不可观测

正是因为隐变量的存在，使得X的对数似然函数会包含$$log(f_1(\theta) + ... + f_k(\theta))$$这样的项，其中$$\theta$$为模型参数。进一步地，这使得对数似然函数难以对$$\theta$$求偏导，也就加大参数求解的难度。EM算法正是用于解决这个问题的，它的E步相当于得到对数似然函数的近似形式，M步相当于对近似形式求极大值解，优美之处在于每一轮E-M迭代都能得到离真实参数解更近的值。EM算法同时也是一种坐标轴下降最优化方法。

但既然隐变量的存在会加大参数求解难度，那为什么还要引入呢？目前没有深究这个问题，但从直觉上看，既然隐变量会影响观测变量，那么不妨将隐变量认为是模型的一维"特征"，即使无法观察到这个特征的真实取值，但至少还能得知它所服从的分布，给模型推断带来一些有用的信息量。

在理论推导时最常见的层次模型是GMM, 即高斯混合模型，虽然在业界应用比较广泛的层次模型应该是LDA.. 

高斯混合模型假设$$X;Z_k$$服从高斯分布，分布参数由$$Z_k$$决定，而$$Z_k$$服从多项式分布。
