---
layout: post
tags: 推荐算法 learning_to_rank
---

这是来自于阅读Steffen Rendle的Factorization Machine论文的笔记

### FM和LR在特征交叉上的对比
FM可以用在推荐系统中, 作用是将特征交叉考虑到模型中。和在LR前做特征交叉相比, FM的优势主要是在性能上。还是以LR为例, 假如要做完整的二阶特征交叉, 那么LR的模型参数会比没有特征交叉时增加$$m(m-1)/2$$个, 但如果是用FM, 增加的项是$$\sum_i^n \sum_{i + 1}^n <v_i, v_j> x_i * x_j$$, 也就是说每个特征对应一个"因子向量", 在做特征交叉时, 每一个新的组合特征的参数都由因子向量的内积得出, 这样就可以将新增参数的数量降为$$m*k$$, 其中$$k$$为因子向量的维度, 一般来说会比原始特征的数目要少。这就是为什么FM比LR做特征交叉的效率更高。
FM的核心思想就如上文所说的"特征交叉", 它可以作为回归模型、分类模型或排序模型来用, 分别可以对应于平方损失函数、hinge损失函数、pair-wise损失函数。在求解时可以用SGD进行求解, 实现来说的话, 只需要将损失函数的梯度表示出来, 就可以调用spark的optimizations模块来完成最优化求解, 从而求得模型参数。

### FM的预测阶段
FM预测阶段的算法复杂度是线性的，准确来说是当特征的分解因子的大小$$k$$确定时，线性复杂度相对于特征维度来说是线性的。
一般来说预测样本上的特征都是比较稀疏的，因此只需要找到非零特征对应的一阶特征权重和二阶特征分解因子权重，再进行线性加权，就可以计算得到预测值。

### FM和MF的关系
从模型形式来看，MF可以看作是FM的特例：就是当FM的特征都是用户ID或物品ID组成的哑变量时，这时候各个特征的分解因子就相当于是这个用户ID或物品ID的隐变量。

