---
layout: post
tags: word2vec nlp
---

word2vec本身所表达的是一个语言模型，词向量理论上是这个语言模型的副产物，但实际上是更为重要的模型产出。因为将词表达成向量空间的一个向量之后，就可以作为其它需要考虑语义的应用的输入，这比单纯依据context预测下一个词的场景要广泛得多。

在word2vec之前有一些通过词频矩阵进行低秩分解的模型，它们同样可以得到词和文档在同一向量空间中的因子向量。word2vec与它们的区别在于，它并不统计词频，而是通过预测的方式来得到词向量，而且本身也没有严格的所谓"文档"的定义，因为word2vec将语料构造成样本时是通过设定词窗口大小，使得词窗口在语料中滑动，从而构造出样本的两部分：context和target，预测的目标正是使得样本的似然函数值最大。

而word2vec同时作为一个神经网络模型，这意味着它的结构并不是从输入直接连接到输出，而是中间通过一个隐藏层进行自动特征抽取，目的是为了提高预测精度。这样一来，再联系上词向量映射的思想，就组成了word2vec模型的主要结构是由四层组成的：输入层、映射层、隐藏层、输出层，以CBOW为例：

- 输入层为context所包含的词集
- 输出层为target词
- 映射层为由上下文词集所得的词向量
- 隐藏层包含了h个神经元，用于

full softmax从映射层到输出层可以看作是一个多类别分类模型，其中输入为词向量，输出为由softmax计算所得的在给定上下文词向量前提下，target词为词汇表中特定某个词的概率，即$$P(w_O; f(V(w_{I_1}), ..., V(w_{I_}))) = $$

### 层次softmax网络结构
### 输入层到映射层
### 映射层到输出层
#### 词编码
#### 

