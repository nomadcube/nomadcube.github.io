---
layout: post
tags: learning-to-rank 最优化
---

FTRL是一种在线学习方法，它的优势如下：

1. 相比起批量学习，它的时效性更高，消耗的计算资源也更少
2. 相比起OGD(Online SGD), 它能在保证模型精度的同时得到稀疏解

以下以LR为例进行阐述。

### **LR损失函数**
定义为交叉熵损失函数：$$L(\vec w) = \sum_i^N -y_ilog\hat y_i - (1 - y_i)log(1 - \hat y_i)$$。

### **OGD参数更新公式**
OGD每次只以当前一条样本的损失函数作为最优化目标，即$$-y_ilog\hat y_i - (1 - y_i)log(1 - \hat y_i)$$, 对应的梯度函数为$$g(\vec w) = (\hat y_i - y_i) \vec x^{(i)}$$, 其中$$\hat y_i = logit(\vec w \vec x^{(i)})$$。

这时候OGD的参数更新公式为$$\vec w^{(t+1)} = \vec w^{(t)} - \alpha_t g(\vec w^{(t)})$$, 其中$$\alpha_t$$为随着迭代次数非递增的学习速率，比如可以定义成$$\frac{1}{\sqrt t}$$。

### **FTRL推导**
FTRL相当于是给OGD加入正则化约束，目的是为了得到稀疏解，因此加的是L1正则项。

得到这个结论要经过两步推导：

1. OGD的等价优化目标函数$$f_{OGD}(w)$$
2. $$f_{OGD}(w)$$加上正则化项之后构成FTRL的最优化目标函数$$f_{FTRL}(w)$$, 由$$f_{FTRL}(w)$$求得FTRL对应的的参数更新公式

详细推导如下：

#### **1) OGD等价于求解$$f_{OGD}(w)$$的极小值点**
上文提到OGD的参数更新公式为$$\vec w^{(t+1)} = \vec w^{(t)} - \alpha_t g(\vec w^{(t)})$$，接下来证明它相当于对$$f_{OGD}(\vec w) = \sum_{s=1}^t \vec g_s \vec w + \frac{1}{2}\sum_{s=1}^t \sigma_s  \left\lVert \vec w - \vec w_s \right\rVert_2^2$$求极小值解：

【1】$$\vec w_{t+1}$$满足令$$f_{OGD}(\vec w)$$求关于$$\vec w$$的导数为零，即$$\sum_{s=1}^t \vec g_s - \sum_{s=1}^t \sigma_s \vec w_s + (\sum_{s=1}^t\sigma_s) \vec w_{(t+1)}$$

【2】同样地，$$\vec w_t$$也满足$$\sum_{s=1}^{t-1} \vec g_s - \sum_{s=1}^{t-1} \sigma_s \vec w_s + (\sum_{s=1}^{t-1}\sigma_s) \vec w_{(t)}$$

【3】将【1】-【2】，得$$\vec g_t - \sigma_t \vec w_t + (\sum_{s=1}^t \sigma_s) \vec w_{t+1} - (\sum_{s=1}^{t-1} \sigma_s) \vec w_{(t)} = 0$$

【4】由【3】得$$\vec w_{(t+1)} = \vec w_{(t)} - \frac{\vec g_t}{\sum_{s=1}^t \sigma_s}$$. 其中根据定义$$\sum_{s=1}^t \sigma_s$$正好是$$\frac{1}{\alpha_t}$$, 因此得证。


#### **2) FTRL的最优化目标函数$$f_{FTRL}$$及对应的的参数更新公式**
FTRL相当于对OGD的最小化目标函数$$f_{OGD}(w)$$加上一个L1范数作为正则项，即$$f_{FTRL}(w) = \sum_{s=1}^t \vec g_s \vec w + \frac{1}{2}\sum_{s=1}^t \sigma_s  \left\lVert \vec w - \vec w_s \right\rVert_2^2 + \lambda \left\lVert \vec w \right\rVert_1$$. 

这时要求FTRL的最小值解，也是需要进行求导。由于FTRL是关于$$\vec w$$的函数，不妨对$$\vec w$$的其中一维分量$$w_i$$求偏导得：

$$\sum_{s=1}^t g_{si} + \sum_{s=1}^t\sigma_s(w_i - w_{si}) + \lambda sgn(w_i) = 0$$, 

于是$$f_{FTRL}(w)$$的极小值解在第i个分量上为$$\frac{\sum_{s=1}^t\sigma_sw_{si} - \sum_{s=1}^tg_{si} - \lambda sgn(w_i)}{\sum_i^t \sigma_s}$$.

### 参考文献
[1] *Ad Click Prediction: a View from the Trenches*

