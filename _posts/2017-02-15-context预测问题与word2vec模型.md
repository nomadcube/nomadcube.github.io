---
layout: post
tags: 神经网络
---

### context预测问题 
典型的context预测问题存在于NLP领域中。语言可以认为是定义所有可能的$$P(w_1, w_2, ..., w_n)$$, 即词汇表中n个词组成的序列的联合概率，也就是说这n个词能够成为一个正常的句子。而如果已知一种语言的分布，那么就可以利用context去预测目标词，也就是预测$$P(w_i;w_1,...,w_{i-1},w_{i+1}...,w_{n})$$. 

### n-gram语言模型
通过上下文去预测目标词的任务，可以通过统计模型来实现。具体来说就是计算语料中这n个词同时出现次数$$a$$，再计算除了目标词以外的n-1个词的出现次数$$b$$，这样就可以用$$\frac{a}{b}$$来估计context预测的概率。

用这种方式去估计所有的context预测概率时会遇到一个问题：维数灾难。为了解决这个问题，可以对问题做一个假设：目标词的出现概率只与它前后的n个词相关，也就是所谓的n-gram语言模型。

### 神经网络语言模型
以上的n-gram语言模型也可以认为是基于counting去做context预测。在2003年由Yoshua Bengio等人提出神经网络语言模型，改变了counting的方式，而是用直接用predict的方式去做context预测。也就相当于用判别模型而不是生成模型。

神经网络语言模型的主要思想有两个：1. 将离散的词ID映射到连续的词向量空间；2. 以词向量拼接后所得的向量作为神经网络的隐层，隐层和输出层之间做成全连接。

### CBOW还是skip-gram?
选用CBOW还是skip-gram相当于决定样本的构造形式。如果是选用CBOW的话，相当于input是context,而output是目标词；如果是选用skip-gram则相反。

### 从full softmax到hierarchical softmax和NCE
构造出训练样本之后，就可以进一步去定义损失函数。最简单的想法是在输出层做softmax, 计算词汇表中任意一个词作为输出的概率。这样做的问题是在训练计算梯度时，需要遍历词汇表中每一个词，这样训练的成本是很高的。

针对这个问题的一个解决思路有两个：hierarchical softmax和NCE。

hierarchical softmax的思想是将词汇表中的每个词表示为二叉树的叶节点，树的root和内结点都表示成词向量的形式，这样计算$$P(w_O;w_I)$$时就不需要用softmax, 也就是说不需要遍历词汇表中全部的词，只需要表示为从$$w_I$$到root, 沿着路径到$$w_O$$所经过的所有node的联合概率。

NCE则利用采样的思想。目标词和context的正样本是直接从语料中构造出来的，但负样本量很大，以window大小设成1为例，这样负样本相当于是目标词和词汇表的其它的n-1个词的组合。NCE的做法是从中进行抽样，只以其中的k个作为真实使用的负样本。预测目标是使得正样本的对数似然值高，而负样本的比较低，概率是依据logistic算出来的。
