---
layout: post
tags: 神经网络
---

### NLP中的context预测问题 
最典型的context预测问题存在于NLP领域中，因为它与语言模型的发展有关。 

首先，我们怎么去定义一种语言呢？如何判断一段话是属于某一种语言的呢？

### n-gram语言模型

### 神经网络语言模型
可以看作是由神经网络模型+词向量映射组合而成的算法。整体的输入是从第t-1个词往前推n个词的索引，输出是第t个词的索引。

将n-1个词映射到m维词向量之后，拼接成一个(n-1)*m维的向量，这个作为神经网络模型的输入层。

接下来是输入层到隐藏层的映射，假设隐藏层有h个神经元，那么到这一层的映射有h+h(n-1)m个参数，其中包括h个偏置项。这一层使用的是tanh激活函数。

输出层是长度为$$|V|$$即词汇量大小的向量。这一层是将隐藏层的h个神经元映射到|V|个输出单元上。使用的是softmax激活函数。

同时，输入层也可以直接连接到输出层。

### word embedding的思想
区分传统机器学习和深度学习一个很重要的特性是，将物体看作是单独的"节点"还是看作是向量表示。

在传统机器学习中，样本的特征通常表示为一个连续或离散的变量值，机器无法识别2个特征之间的关系（最多只能通过统计这2个特征的共现等方式去看这2个变量之间的关系）。但在深度学习中，可以将特征表示为一个向量，这样就可以通过向量的距离计算来得知这2个特征的关系了。

### CBOW还是skip-gram?

### 从full softmax到hierarchical softmax和NCE

