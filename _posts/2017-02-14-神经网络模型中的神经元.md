---
layout: post
tags: 神经网络
---

在神经网络模型中，除了输入层之外，其它的层都可以看作是由神经元组成。
神经元指的以上一层的节点作为输入，按一定的规则输出值。常见的神经元类型分为以下三种：

1. 线性神经元，即激活值为输入的线性加权形式
2. 二元神经元，即根据一个阈值将线性加权值映射到0或者1
2. sigmoid神经元。激活函数是sigmoid函数，如logit, tanh函数

另一方面，神经网络模型的最初始的输入也不一定是传统机器学习那样子的特征向量。比如在神经网络语言模型中，模型的输入是词在词汇表中的索引，在下一层才映射成词向量，作为每个词的特征。

人工神经网络模拟的是神经元之间的信息传递, 而只有被激活的神经元才会给其它神经元发送信息。因此怎么定义激活状态是神经网络中的其中一个重点。

用数学语言来描述的话, 可以用1来代表被激活的状态, 用0来代表不被激活的状态, 激活规则表示成一个映射函数, 最简单的是跃阶函数```sign(x)```, 即当x为正数时取值1, 即被激活, 否则取值0, 即不被激活。

但在实际应用中, ```sign(x)```由于不连续因此用得较少, 较常用的是sigmoid函数, 它和```sign(x)```相同的性质是值域在0,1之间, 不同之处在于, sigmoid函数不会在0点处跳跃, 而是得到一个连续值, 从0连续递增到1. 这样同样可以决定神经元的激活状态, 只不过并不是绝对的"被激活"或"不被激活", 而是用$$(0, 1)$$之是的实数值表示被激活的程度, 这样所有神经元都会对其它神经元发送消息, 只不过根据激活程度, 发送消息的程度不一致。

最经典的sigmoid函数是logit函数$$\frac{1}{1 + e^{-x}}$$, 而其它的```tanh(x)```、softmax函数也属于广义的sigmoid函数。因此它们都可以用作神经网络的激活函数。

但在输出层的时候，相比起sigmoid，softmax会更加常用。因为sigmoid适用于二分类问题，而softmax由于是一个标准化的exp(x)函数，因此很容易从二分类问题扩展到多分类问题。而softmaxe用在神经网络上有一个很好的性质是，增减一个神经元，对exp(x)的影响是成倍数级别的。

当输出层用softmax时，损失函数一般用交叉熵来定义。交叉熵就是对数似然，对数似然就是交叉熵。啊啊啊。
