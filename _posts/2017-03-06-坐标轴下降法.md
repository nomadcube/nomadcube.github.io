---
layout: post
tags: 机器学习
---

核心思想在于，对于某些有p个参数的最优化问题，固定其中p-1个之后，最优化目标函数就变成了其中那1个参数的二次函数，因为可以求出解析解，用这个解析解作为下一轮迭代的参数值。

此过程在p个参数上轮流迭代，直至收敛。

和梯度下降的最大区别在于，梯度下降每次都需要求出参数向量在当前轮迭代或当前样本上的梯度值，这个梯度值也是长度为p的向量，然后在步长alpha下进行迭代更新。
而坐标轴下降每次更新时算的不是向量，而是在当前这个没被固定的参数上的"最优解"。

