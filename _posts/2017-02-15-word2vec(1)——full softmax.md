---
layout: post
tags: 语言模型 神经网络
---

word2vec在2003年由Yoshua Bengio等人在论文*A Neural Probabilistic Language Model*中提出。作为语言模型的其中一种，它与其它常见语言模型的区别在于：

1. 不基于词袋模型，因此也不需要统计词频
2. 需要考虑词料中词条之间的顺序（但一旦根据词序构造好上下文和target词，上下文内部的词序就不重要，之后会提到）
3. 用判别模型的形式进行预测，而不是用生成模型的形式来计算联合概率分布

词向量理论上是这个语言模型的副产物，但从实际应用上看反而词向量是更为重要的模型产出。因为将词表达成向量空间的一个向量之后，就可以作为其它需要考虑语义的应用的输入，这比单纯依据context预测下一个词的场景要广泛得多。

之前有一些通过词频矩阵进行低秩分解的模型，它们同样可以得到词和文档在同一向量空间中的因子向量。word2vec与它们的区别在于，它并不统计词频，而是通过预测的方式来得到词向量，而且本身也没有严格的所谓"文档"的定义，因为word2vec将语料构造成样本时是通过设定词窗口大小，使得词窗口在语料中滑动，从而构造出样本的两部分：context和target，预测的目标正是使得样本的似然函数值最大。

### **基本网络结构**
而word2vec同时作为一个神经网络模型，这意味着它的结构并不是从输入直接连接到输出，而是中间通过一个隐藏层进行自动特征抽取，用于提高模型复杂度和预测精度。这样一来，再联系上词向量映射的思想，就组成了word2vec模型的四个层：输入层、映射层、隐藏层、输出层。以CBOW为例，输入层为context所包含的词集；映射层为由上下文词集所得的词向量；隐藏层包含了h个神经元，用于将词向量进行进一步抽象；输出层为target词的概率，是隐藏层和target词的全连接，如下图所示：

![word2vec_1](/public/word2vec_1.png)

其中embedding阶段可以认为是模型的核心部分。因为在这个模型提出之前几乎所有语言模型都用符号来表示词，词与词之间的关系用通过共现频率反映，而word2vec模型的改进点在于，用连续向量来代表词，词之间的关系用向量距离进行衡量。可见embedding给模型带来了$$VK$$个参数，其中$$V$$为词汇量大小，$$K$$为向量长度。

而在full connect阶段，则将预测问题看成是多类别分类问题，输出层的类别个数是$$V$$, 也就是需要预测词汇量大小个类别的概率，同时embed得到的词向量成为网络结构的隐藏层。这时候连接隐藏层和输出层的参数个数为$$VKV$$。

### **full softmax**
在使用BP对模型参数进行求解时会有一个问题：在模型前向传播时，输出层"似乎"只有一个词和隐藏层的连接会被用到，也就是说"似乎"隐藏层和连接层的$$VKV$$当中在一次迭代时只需要更新其中的$$KV$$个。但实际上并不是这样的。当我们使用softmax去解决这个多类别分类问题时，输出层每一个词的概率都会被表示成相对于隐藏层的$$VKV$$个参数的表达式，因为softmax是需要在分母加标准化项的：$$\frac{e^{x_i}}{\sum_j e^{x_j}}$$, 分母中的$$\sum_j e^{x_j}$$实际上是需要用到输出层每一个可能的词与隐藏层的连接的。

![word2vec_2](/public/word2vec_2.png)

以上这种求解方式也称为full softmax, 导致每次迭代都需要对$$VKV+VK$$个参数进行求梯度和参数更新，会影响模型训练性能。为了解决这个问题，目前主要使用的两种思路分别是hierarchical softmax和NCE。

### 参考文献

- *A Neural Probabilistic Language Model*
- *Distributed Representations of Words and Phrases and their Compositionality*
- *Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors*
