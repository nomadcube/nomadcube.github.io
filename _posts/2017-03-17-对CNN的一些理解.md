---
layout: post
tags: 深度学习
---

典型的卷积神经网络层的类型有这些：卷积层conv、激活层relu、池化层pooling、全连接层fc. 

输入层表示成四维张量：batch_size, 图长, 图宽, channel数目（图深）。卷积层的意义在于将这个四维张量映射到另外一个四维张量，而用作映射的线性算子也是一个四维的张量：图长、图宽、channel数目、映射后的channel数目即filter数目。卷积从直观上理解是一个加权求和的过程，在CNN里面相当于filter权重和每次移过的一个局部区域的点积。这里的filter从概念上来看比较接近于kernal, 也就是说作用是来做线性映射的。一个理解上的转折点是，在做卷积时并不是预先将整个图像切分成局部的几块，而是预先有K个filter, 每个filter在图像上移过去一个filter大小的区域都做一次点积，得到一个值。这样有K个filter的话，就可以将这K个激活值"重叠"在一起。

relu则是指以max(0, x)作为激活函数的神经元，lu的意思是"线性神经元"，即当前层的神经元是前一层神经元的线性组合，max(0, x)应用在这个线性组合的值上面，是一个element-wise的操作。

而池化层则是指从卷积后的四维张量中对长*宽做抽样，常用的是按a*a做max抽样。这样的作用是降低网络复杂度和参数空间大小。

全连接层是在池化之后参数比较少了之后做一次两层之间的全连接操作。
