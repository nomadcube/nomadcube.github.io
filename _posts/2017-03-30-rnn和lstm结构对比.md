---
layout: post
tags: 深度学习
---

### 序列预测问题
从[word2vec发展史](https://nomadcube.github.io/2017/02/15/word2vec%E5%8F%91%E5%B1%95%E5%8F%B2/)中我们可以看出word2vec也是可以用于语言模型的预测的，也就是说给定一个句子前几个词，预测出下一个词是什么。但这和序列模型的区别在于，word2vec的输入是无序的，只要能代表"上下文集合"就可以了。而RNN或LSTM模型一个改进点是输入是有序的，输入元素之间的先后顺序也考虑到模型之中。

### RNN
CNN的问题在于网络结构是确定的，也就是说它的输入和输出是固定大小的。这对于序列来说是不合理的，比如文本序列、视频的各帧等。这时候就需要用到RNN。RNN的关键思想在于它会将序列中的每个元素都构建一个网络，相依的网络之间还进行连接，也就是说上一层网络的输出是下一层网络的输入。

这样做的好处是使得网络有"记忆"。但这也只是很短期的记忆，只是基于上一个阶段所产生的记忆。

*Understanding LSTM Networks*[1] 文中给出的清晰明了的单隐藏层RNN结构：
![single-rnn](/public/single-rnn.png)

解释一下：

1. 单隐藏层是指从x到h只有一个隐藏层，这里隐藏层的概念和一般NN的是一致的，比如有带有隐藏层神经元个数这个超参数hidden_size
2. 图中所显示的是一个循环3次的RNN，对应于tensorflow中RNN模型的超参数num_steps=3
3. 传统RNN下，网络结构的记忆由隐藏层进行传递，也就是说第t个时刻的隐藏层接收的输入，除了当前的输入值$$x_t$$之外，还有前一个时刻的隐藏层值$$h_{t-1}$$. 用数学公式表达就是$$h_t = tanh(Wx_t + Uh_{t-1})$$, 其中W和U分别代表当前时刻输入和前一个时刻记忆的权重，最后会根据输出层的损失函数来修正这些权重

### LSTM
RNN的问题在于它不能记住太久之前的信息，而LSTM则又是来解决长期记忆问题的。它是可以将相隔多个阶段的网络之间连接起来，用一些读、写、是否忘记的阀门来决定网络形态。

LSTM和RNN最大的区别在于序列之间通过4层的神经网络去传递信息，而RNN只有一层。这4层网络共同组成了3个阀门，具体见下图（还是来自于*Understanding LSTM Networks*[1]）：

![lstm](/public/lstm.png)

"阀门"的作用是来控制每一类信息往下传递的强度，强度用$$[0,1]$$之间的系数来表示，3个阀门的系数分别对应于图中的3个带$$\sigma$$操作符号的矩形框。从中可以看到每个$$\sigma$$矩形框都作为一个点乘框的输入，这也代表着点乘操作的左算子是由$$\sigma$$计算出来的$$[0,1]$$之间的系数，而右算子则是3个阀门对应的"信息"。这3种信息分别是：

1. 历史信息，即上一次循环得到的值
2. 历史信息和当前信息共同得到的值
3. 1和2经过阀门系数加权求和的结果

从图中也可看出各个阀门的系数都是关于上一次循环所得的值$$h_t$$和当前循环输入值$$x_t$$的函数，具体是将这两个向量进行拼接。

"阀门"可以认为是LSTM的核心思想，因为阀门系数从0到1代表着每类信息是否会被记住。举个例子，假如图中最左边的$$\sigma$$和点乘组成的阀门中系数为0，这就意味着需要将单纯由上一次循环产生的信息忘记。

这三个阀门分别对应于以下这三个图（还是来自*Understanding LSTM Networks*[1]，毕竟是难得的好文）：

![lstm1](/public/lstm1.png)

![lstm2](/public/lstm2.png)

![lstm3](/public/lstm3.png)

### 参考文献
[1] [*Understanding LSTM Networks*](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[2] [*A Beginner’s Guide to Recurrent Networks and LSTMs*](https://deeplearning4j.org/lstm)
