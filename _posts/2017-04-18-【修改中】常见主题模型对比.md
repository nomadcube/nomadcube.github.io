---
layout: post
tags: 主题模型
---

同样作为生成模型，主题模型和unigram的最大区别在于引入"主题"的概念，即假设文档的生成是先确定它的主题，再根据主题生成词条。而这里的"主题"则对应于隐变量，真实的文档中的词条则对应于可观测变量。

一般主题模型对于"主题-词条"的关系假设都是一致的：各主题是关于词条的分布，不同主题之间分布的参数不同。这些主题模型之间本质的区别在于对主题生成机制的假设。

下面简述mixture of unigrams、pLSA、LDA三者在主题生成机制上的区别。

### **mixture of unigrams**
认为主题的分布与文档无关，而是语料层面的性质，即假设有K个主题，那么从一个文档在主题上的分布参数都是$$(p_1, ..., p_K)$$. 如下图所示：

![unigrams](/public/unigrams.png)

### **pLSA**
与mixture of unigrams有两个区别：

1. 认为主题的分布和文档一一对应，即每个文档在主题上的分布参数不一样
2. 每篇文章的生成也是服从一个概率分布，从中抽样得到

如下图所示：
![plsa](/public/plsa.png)

从中也可看出，pLSA只能对训练样本中的文档起效，因为对于不在训练样本出现的文档，它无法得知文档的出现概率，更无法得知这篇新文档的主题分布。

### **LDA**
暂时先忽略参数学习角度而只从模型角度来看的话，LDA是用于解决pLSA中对新文档进行inference的问题的。解决的方式是通过先验分布，即认为文档的主题分布服从多项分布，其中多项分布的参数也是变量，服从的是狄利克雷分布，即以狄利克雷分布作为某文档的主题的先验分布。

这样的作用是，只要先验分布的参数已知，就可以得到任意文档的主题分布参数，而不需要在意文档是否在训练样本中。

### **参考资料**
[1] *Latent Dirichlet Allocation*
