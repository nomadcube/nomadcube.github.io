---
layout: post
tags: 神经网络
---

在2003年由Yoshua Bengio等人提出神经网络语言模型。神经网络语言模型的主要思想有两个：1. 将离散的词ID映射到连续的词向量空间；2. 以词向量拼接后所得的向量作为神经网络的隐层，隐层和输出层之间做成全连接。

选用CBOW还是skip-gram相当于决定样本的构造形式。如果是选用CBOW的话，相当于input是context,而output是目标词；如果是选用skip-gram则相反。

构造出训练样本之后，就可以进一步去定义损失函数。最简单的想法是在输出层做softmax, 计算词汇表中任意一个词作为输出的概率。这样做的问题是在训练计算梯度时，需要遍历词汇表中每一个词，这样训练的成本是很高的。

针对这个问题的一个解决思路有两个：hierarchical softmax和NCE。

hierarchical softmax的思想是将词汇表中的每个词表示为二叉树的叶节点，树的root和内结点都表示成词向量的形式，这样计算$$P(w_O;w_I)$$时就不需要用softmax, 也就是说不需要遍历词汇表中全部的词，只需要表示为从$$w_I$$到root, 沿着路径到$$w_O$$所经过的所有node的联合概率。

NCE则利用采样的思想。目标词和context的正样本是直接从语料中构造出来的，但负样本量很大，以window大小设成1为例，这样负样本相当于是目标词和词汇表的其它的n-1个词的组合。NCE的做法是从中进行抽样，只以其中的k个作为真实使用的负样本。预测目标是使得正样本的对数似然值高，而负样本的比较低，概率是依据logistic算出来的。
