---
layout: post
tags: 神经网络
---

在2003年由Yoshua Bengio等人提出神经网络语言模型。神经网络语言模型基本结构可以看成是一个3层的神经网络。以CBOW为例，输入层为context中的词ID，输出层为目标词ID对应的概率值，隐藏层为context词ID映射而成的词向量。因此参数也分成2部分：词汇表中每个词的embedding向量权重；从隐藏层到输出层的连接权重。

以下为bengio论文给出的神经网络语言模型的基本结构：
![word2vec_bengio](/public/word2vec_bengio.png)

当输出层定义为在词汇表上所有词的概率时，概率可以用softmax定义，即$$\frac{e^{S_{w_i}}}{\sum_j e^{S_{w_j}}}$$, 其中$$S_{w_i}$$代表的意义输出为$$w_i$$的得分，也就是隐藏层各个神经元的激活值和隐藏层到输出层权重的内积。从中我们也可以看出，隐藏层到输出层的权重为$$nkv$$大小的张量, 其中nk为隐藏层的大小，即context的长度和词向量长度的乘积，v则是输出层的大小，也就是词汇量的大小。

这时候我们在定义模型的损失函数时就会用到输出层每个词的softmax值，由于softmax的分母是所有词的指数得分加总，因此在计算损失函数或者说梯度时需要遍历词汇表中的每一个词，这就导致模型训练时的性能低下。

目前解决这个问题的思路有两个：hierarchical softmax和NCE。

hierarchical softmax的思想是将词汇表中的每个词表示为二叉树的叶节点，树的root和内结点都表示成词向量的形式，这样计算$$P(w_O;w_I)$$时就不需要用softmax, 也就是说不需要遍历词汇表中全部的词，只需要表示为从$$w_I$$到root, 沿着路径到$$w_O$$所经过的所有node的联合概率。

NCE则利用采样的思想。目标词和context的正样本是直接从语料中构造出来的，但负样本量很大，以window大小设成1为例，这样负样本相当于是目标词和词汇表的其它的n-1个词的组合。NCE的做法是从中进行抽样，只以其中的k个作为真实使用的负样本。预测目标是使得正样本的对数似然值高，而负样本的比较低，概率是依据logistic算出来的。
