---
layout: post
tags: 深度学习
---

### 序列预测问题
从[word2vec发展史](https://nomadcube.github.io/2017/02/15/word2vec%E5%8F%91%E5%B1%95%E5%8F%B2/)中我们可以看出word2vec也是可以用于语言模型的预测的，也就是说给定一个句子前几个词，预测出下一个词是什么。但这和序列模型的区别在于，word2vec的输入是无序的，只要能代表"上下文集合"就可以了。而RNN或LSTM模型一个改进点是输入是有序的，输入元素之间的先后顺序也考虑到模型之中。

### RNN
CNN的问题在于网络结构是确定的，也就是说它的输入和输出是固定大小的。这对于序列来说是不合理的，比如文本序列、视频的各帧等。这时候就需要用到RNN。RNN的关键思想在于它会将序列中的每个元素都构建一个网络，相依的网络之间还进行连接，也就是说上一层网络的输出是下一层网络的输入。

这样做的好处是使得网络有"记忆"。但这也只是很短期的记忆，只是基于上一个阶段所产生的记忆。

*Understanding LSTM Networks*[1] 文中给出的清晰明了的单隐藏层RNN结构：
![single-rnn](/public/single-rnn.png)

解释一下：

1. 单隐藏层是指从x到h只有一个隐藏层，这里隐藏层的概念和一般NN的是一致的，比如有带有隐藏层神经元个数这个超参数hidden_size
2. 图中所显示的是一个循环3次的RNN，对应于tensorflow中RNN模型的超参数num_steps=3
3. 传统RNN下，网络结构的记忆由隐藏层进行传递，也就是说第t个时刻的隐藏层接收的输入，除了当前的输入值$$x_t$$之外，还有前一个时刻的隐藏层值$$h_{t-1}$$. 用数学公式表达就是$$h_t = tanh(Wx_t + Uh_{t-1})$$, 其中W和U分别代表当前时刻输入和前一个时刻记忆的权重，最后会根据输出层的损失函数来修正这些权重

### LSTM
而LSTM则又是来解决短期长期记忆的问题。它是可以将相隔多个阶段的网络之间连接起来，用一些读、写、是否忘记的阀门来决定网络形态。

### 参考文献
[1] [*Understanding LSTM Networks*](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[2] [*A Beginner’s Guide to Recurrent Networks and LSTMs*](https://deeplearning4j.org/lstm)
