---
layout: post
tags: 最优化
---


在求解模型最优解时，实际上是求所定义的损失函数的最小值解。
假设以SGD为最优化算法，那么在每一次迭代时都需要计算损失函数的梯度，准确来说是损失函数对于模型参数的梯度。而根据链式法则，损失函数相对于模型参数的梯度可以分解为"损失函数相对于模型的梯度"与"模型相对模型参数的梯度"的乘积。
而我们都知道常用的损失函数就那么几种：平方损失函数、最大似然损失函数等。而这些损失函数相对于模型的梯度的形式是比较简单的。
另一方面，不管使用哪种损失函数，模型相对模型参数的梯度都是不随之变化的。因此对同一个模型使用不同的损失函数求解时，只需要将"损失函数相对于模型的梯度"与"模型相对模型参数的梯度"的乘积中的第一部分进行修改。


缘于最近看到的一篇介绍反向传播算法的文章（http://cs231n.github.io/optimization-2/），第一次见到如些通俗易懂的介绍方式。本篇是从中得到的一些感悟和理解。

反向传播算法实质上是在求损失函数关于各层参数的偏导，而得到的向量形式的"偏导"就是我们所说的"梯度"。这在本质上和经典机器学习是一致的，但为什么非得要叫"反向传播"呢，原因就在于DNN具有一个或多个隐藏层，这就意味着在输出层和真实观测比对所得到的损失函数是关于各层参数的复合函数，相当于```loss(x) = f1*f2*f3(x)```之类的。这时候求损失函数相对于参数x的梯度就需要用链式法则对复合函数求导了，"反向传播"可以看成是对这个过程的一个形象的描述。

另一方面，"反向传播"同时也反映了这么一个事实：最终损失函数的"变动"会沿着网络的反方向回传到前面各层，相当于将对误差的影响分摊给前面各层的所有参数。

而在DNN中，```loss(x) = f1*f2*f3(x)```里面涉及到的这些函数形式一般与网络结构的定义有关，比如神经元所使用的激活函数是logit, softmax, max还是tanh等，这些激活函数就决定了其中一个复合函数的形式。另外在CNN中卷积和池化的形式也组成了这些复合函数的其中一些形式。

而以这种方式去理解又是与tensorflow中涉及到的一个"计算图"的概念是一致的。因为可以将每个复合函数看成是图结构中的一个节点，输入输出可以是x, y, w, b之类的张量（tensor）.
