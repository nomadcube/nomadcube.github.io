---
layout: post
tags: 深度学习
---

缘于最近看到的一篇介绍反向传播算法的文章（http://cs231n.github.io/optimization-2/），第一次见到如些通俗易懂的介绍方式。本篇是从中得到的一些感悟和理解。

反向传播算法实质上是在求损失函数关于各层参数的偏导，而得到的向量形式的"偏导"就是我们所说的"梯度"。这在本质上和经典机器学习是一致的，但为什么非得要叫"反向传播"呢，原因就在于DNN具有一个或多个隐藏层，这就意味着在输出层和真实观测比对所得到的损失函数是关于各层参数的复合函数，相当于```loss(x) = f1*f2*f3(x)```之类的。这时候求损失函数相对于参数x的梯度就需要用链式法则对复合函数求导了，"反向传播"可以看成是对这个过程的一个形象的描述。

另一方面，"反向传播"同时也反映了这么一个事实：最终损失函数的"变动"会沿着网络的反方向回传到前面各层，相当于将对误差的影响分摊给前面各层的所有参数。

而在DNN中，```loss(x) = f1*f2*f3(x)```里面涉及到的这些函数形式一般与网络结构的定义有关，比如神经元所使用的激活函数是logit, softmax, max还是tanh等，这些激活函数就决定了其中一个复合函数的形式。另外在CNN中卷积和池化的形式也组成了这些复合函数的其中一些形式。

而以这种方式去理解又是与tensorflow中涉及到的一个"计算图"的概念是一致的。因为可以将每个复合函数看成是图结构中的一个节点，输入输出可以是x, y, w, b之类的张量（tensor）.
