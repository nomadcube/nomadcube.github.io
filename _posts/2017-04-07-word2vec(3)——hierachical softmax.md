---
layout: post
tags: 语言模型 神经网络 embedding
---

在[NNLM及一些变式](https://nomadcube.github.io/2017/04/10/word2vec(1)-NNLM%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%8F%98%E5%BC%8F/)中提到 hierarchical softmax，用层次化的思想解决full softmax的NNLM模型训练性能低下的问题。

所谓的"层次化思想"和日常的直觉也是相符的，举个通俗的例子：假如有朋友让你猜猜他是谁，要是将所有认识的人的名字过一遍判断是否猜对，是比较浪费时间的，比较好的方法是先判断"是否同学"，若猜中，再进一步判断"是否小学同学"，若不是，继续判断"是否中学同学"，如此类推，直到倒数一个条件得到的信息为"是高中时的基友"，而你高中只有2个基友，那么最可以将范围缩小到这2个人身上，做最后一次判断就可以得知真实信息。

hierarchical softmax做的也是类似的事情：将V个原始词汇置于二叉树的叶节点上，由下往上每个内节点都代表范围更大的一个类，直到根结点，是一个包含V个词的最大的类别。

一旦这样的一棵树已经构造出来，就可以反过来从根结点开始往下一层层地判断，直到到达了词对应的叶结点。这也相当于沿着某个特定词对应的从根结点开始的路径进行预测，每一次预测都是判断"是否属于第这个大类"，因此很明显对应于一个二分类问题。

于是hierarchical softmax就可以总结成如下过程：

1. 对每个词进行编码，每一位取值0或1，长度代表从根结点到它所在叶结点的路径，每一位的物理意义是一个大类，如下图所示，每一位从左到右代表范围从大到小的类别：

![hierarchical softmax](/public/hierarchical%20softmax.png)

2. 计算给定context前提下每串编码的概率。这需要用到一个性质：每一位编码为0或1只取决于它左边的位。这时候的关键是将"位"也就是二叉树的结点表示为和词向量同样长度的向量。这从物理意义上也讲得通，因为既然内结点代表的是一些词所在的大类，那么也可以表示成词向量空间中的一个点，也就是一个相同长度的向量

3. 根据1和2计算编码的概率，也就是每个(context, target)对的似然函数：$$\prod_2^p P(C_i^{x_O} = 1; \vec x_{w_O}, \theta_{i-1}^{w_O})$$. 其中:1）$$w_O$$代表target词，也就是叶结点；2）$$\theta_{i-1}$$代表上一个结点的向量表示；3）\vec x_{w_O}代表输入context投影后得到的向量；4）连乘的意义是对每个结点的概率进行联合

hierarchical softmax和full softmax或NCE在参数空间上最大的区别是没有层之间连接的权重参数，比如softmax或logit的参数。但有2套向量参数，一套面向叶结点，即原始的词汇，另一类面向内结点。

### 参考文献
- *Hierarchical Probabilistic Neural Network Language Model*
