---
layout: post
tags: NLP
---

句法结构分析是基于结构语法的（construction grammar），结构语法也称为上下文无关语法的（Context-Free Grammar, 也就是CFG）。

CFG是一种语法模型，可以表示为$$G=(N, \Sigma, R, S)$$

- $$N$$是指非词条组成的句子成分集合，如NP、VP等
- $$\Sigma$$表示词条组成的句子成分集合，如英文中的单词或中文中的词条
- R指句子成分间的解析规则集合，如$$S -> NP,VP$$代表的是S这种成分可以分解成NP，后面跟着VP
- S是指句子的初始符号，直观上来看是指没被解析的完整句子

可见，CFG可以将一个句子解析成$$s_1 s_2...s_n$$这样的序列，其中$$s_1=S$$，$$s_n \in \Sigma$$, 中间的$$s_i$$可以包含词条或非词条的句子成分。这相当于将一个句子的句法结构按从左到右的方法解析出来，因此这个过程称为left-most derivation, 得到的序列$$s_1 ... s_n$$可以表示成树状结构，因为也称为句法结构树。

PCFG和CFG最大的区别在于，前者给R中的元素，即句子成分间的解析规则，赋予了概率值。这也是为什么它名为Probabilistic Context-Free Grammar.

引入概率的其中一个意义在于解决句法结构歧义的问题。在解析规则集R中，会有一些规则的左部是相同而右部是不同的，实际含义是在一种语言中，同样的句子成分解析成另外多种更细粒度的成分的方式不只一种。在这种情况下，同样的一个句子就可能被解析成多个句法结构树，而且无法确定哪种句法结构更为合理。引入概率值后，就可以计算出将同一个句子解析成不同句法结构树所对应的概率值，从而认为概率值大的句法结构分析树更为合理。

PCFG的一此记号如下：

1. $$T_G(s)$$, 代表句子s所有可能的句法结构树集合
2. $$p(t), t \in T_G(s)$$, 代表任意一个句法结构树的概率值
3. $$t^* = argmax_{t \in T_G(s)} p(t)$$, 特定一个句子s的最优句法结构树，求解过程也称为句子s的解码/解析问题

关键在于$$p(t)$$的定义。可以看到t是从G中得到的，$$p(t)$$的定义依赖于G中的R, 即句子成分分解规则。在PCFG给R中的元素赋予了概率值后，$$p(t)$$的定义就很自然了：$$p(t) = \product_i^n q(\alpha_i -> \beta_i)$$, 其中$$\alpha_i -> \beta_i$$是指s解析过程中包含的R中的规则。

另一个问题是如何得到一个语法模型，即如何求出$$N, \Sigma, R, S$$。

这其实是一个有监督学习问题，训练数据是标注好的句法结构树如$$(t_1, t_2, ..., t_m)$$

$$N, \Sigma$$很简单，分别是所有句法结构树中出现的词条/非词条成分。

至于R，可以用极大似然估计去求解，即$$q(\alpha -> \beta) = \frac{count(\alpha -> \beta)}{count(\alpha)}$$

参考资料：

http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/pcfgs.pdf
http://nlp.stanford.edu:8080/parser/index.jsp
http://nlp.stanford.edu/software/lex-parser.shtml
http://www.hankcs.com/nlp/chinese-sentences-svo-java-extraction.html
