---
layout: post
tags: 深度学习
---

### **full softmax**
在2003年由Yoshua Bengio等人在论文*A Neural Probabilistic Language Model*中提出神经网络语言模型。神经网络语言模型基本结构可以看成是一个3层的神经网络。以CBOW为例，输入层为context中的词ID，输出层为目标词ID对应的概率值，隐藏层为context词ID映射而成的词向量。因此参数也分成2部分：词汇表中每个词的embedding向量权重；从隐藏层到输出层的连接权重。

以下为bengio论文给出的神经网络语言模型的基本结构：
![word2vec_bengio](/public/word2vec_bengio.png)

当输出层定义为在词汇表上所有词的概率时，概率可以用softmax定义，即$$\frac{e^{S_{w_i}}}{\sum_j e^{S_{w_j}}}$$, 其中$$S_{w_i}$$代表的意义输出为$$w_i$$的得分，也就是隐藏层各个神经元的激活值和隐藏层到输出层权重的内积。从中我们也可以看出，隐藏层到输出层的权重为$$nkv$$大小的张量, 其中nk为隐藏层的大小，即context的长度和词向量长度的乘积，v则是输出层的大小，也就是词汇量的大小。

这时候我们在定义模型的交叉熵损失函数时就会用到输出层每个词的softmax值，由于softmax的分母是所有词的指数得分加总作为一个标准化项，因此在计算损失函数或者说梯度时需要遍历词汇表中的每一个词，这就导致模型训练时的性能低下。

目前解决这个问题的思路有两个：hierarchical softmax和NCE。

### **hierarchical softmax**
hierarchical softmax则是由Frederic Morin和Yoshua Bengio在2005年的论文*Hierarchical Probabilistic Neural Network Language Model*中提出。提出的原因显然是为了解决full softmax模型的训练性能低下问题。

基本思想是将$$P(v; w_{t-1}, ..., w_{t-n+1})$$进行分解。在分解之前，计算这个概率需要遍历词汇表。

hierarchical softmax所采用的分解方式是，将v表达成一棵平衡树的一个叶节点，这棵平衡树的每个内结点都是0或1，代表着是左子树还是右子树。这样做实质上是给每个词进行编码，编码的长度最大为m，也就是$$log(V)$$. 同时，每个内结点也可以沿着root表示成由0和1组成的一组编码。这样做的好处是，可以将$$P(v; w_{t-1}, ..., w_{t-n+1})$$按v的每一位编码进行分解了：$$P(v; w_{t-1}, ..., w_{t-n+1}) = \prod_j^m P(b_j(v);b_1(v),...,b_{j-1}(v);w_{t-1},...,w_{t-n+1})$$

分解之后，累乘中的每一项都是一个logit表达式，代表的是v的每一位编码取值为1的概率，看作是一个二分类问题，这样就不存在使用softmax时需要遍历词汇表算标准化项的问题。

但这又带来另一个问题：针对每一位的编码都需要有单独的一个logit表达式，也就是说一组独特的参数。这就导致参数空间过大或模型过拟合的倾向。

为了解决这个新的问题，他们又提出"参数共享"的概念。具体做法是，将树的内结点也看成是一个有意义的"词"，和词汇表里的词同等对待，唯一的差别在于查找这些内结点的embedding向量时对应的矩阵是另外一个。这样做的好处是将$$P(v; w_{t-1}, ..., w_{t-n+1}) = \prod_j^m P(b_j(v);b_1(v),...,b_{j-1}(v);w_{t-1},...,w_{t-n+1})$$变成已知context和内结点的向量表示时，求每个内结点为1的概率。这就相当于内结点和目标词分别对应的embedding向量的内积了。

于是，hierarchical softmax就将$$P(w_O;w_I)$$的计算从一个softmax表达式简化成$$\prod_{j=1}^m logit(u_{n(w,j)} * v_{w_I})$$, 其中u代表内结点的embedding向量，v代表叶结点的embedding向量，$$n(w,j)$$表示从root到词$$w_I$$路径上的第j个结点。也就是说，转换成了目标词向量与其路径上各个内结点向量的点积之连乘。

### **NCE**
NCE则利用采样的思想。目标词和context的正样本是直接从语料中构造出来的，但负样本量很大，以window大小设成1为例，这样负样本相当于是目标词和词汇表的其它的n-1个词的组合。NCE的做法是从中进行抽样，只以其中的k个作为真实使用的负样本。预测目标是使得正样本的对数似然值高，而负样本的比较低，概率是依据logistic算出来的。

### 参考文献

- *A Neural Probabilistic Language Model*
- *Distributed Representations of Words and Phrases and their Compositionality*
- *Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors*
- *Hierarchical Probabilistic Neural Network Language Model*
- *Learning word embeddings efficiently with noise-contrastive estimation*
