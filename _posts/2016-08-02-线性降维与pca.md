---
layout: post
tags: 机器学习
---

线性降维从线性代数的角度来看，是对原空间中的样本点做线性变换，得到在另一个低维子空间中的投影。

线性变换矩阵的形式取决于降维的目标。比如mds的目标是使得给定两个原样本点，它们之间的距离在降维前后保持不变，而pca的目标则是使得降维后最大程度在保留原样本包含的信息，同时样本点之间在新子空间中最大程度地分开。于是mds和pca算法最大的区别在于，前者对原样本集的距离矩阵做特征值分解，而后者对原样本集的协方差矩阵做特征集分解。

### PCA的几何意义
一个矩阵代表一个线性变换，线性变换的作用在于将原空间中的向量进行伸缩和旋转，旋转的形式由线性变换的方向决定。而线性变换同时也代表了一个新坐标系，因此"线性变换的方向"就对应于"新坐标系的坐标轴方向"，因此要确定一个线性变换的方向，相当于将它的坐标轴方向找出来。

另一方面，对一个坐标系来说，坐标轴上的单位向量具备的一个性质是，位于坐标系上的线性变换对坐标轴只起伸缩作用，不起旋转作用。以二维坐标系为例，对于坐标轴$$(0, 1)$$，坐标系上的一个线性变换$$((2, 0), (0, 3))$$, 这个线性变换作用在坐标轴$$(0, 1)$$上之后，效果是让$$(0, 1)$$拉伸成$$(0, 3) = 3 * (0, 1)$$, 相当于拉伸了3倍的长度。

综合以上两点可得，要找到一个坐标系的坐标轴方向，相当于找到那些"坐标系只产生伸缩效果，不产生旋转效果"的单位向量。于是这就是pca的理论基础$$Av  = \lambda v$$, 其中$$A$$代表要做pca的矩阵，$$v$$代表$$A$$对应的线性变换的坐标轴方向，即特征向量，$$\lambda$$代表$$A$$对各个特征向量的伸缩比例，对应于特征值。从中也可看出，某个坐标轴被这个线性变换作用之后，伸缩的比例越大，表示这个坐标轴越重要，也就对应于pca的性质之一：特征值大的特征向量更重要。

### PCA求解
