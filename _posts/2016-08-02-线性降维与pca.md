---
layout: post
tags: 机器学习
---

线性降维从线性代数的角度来看，是对原空间中的样本点做线性变换，得到在另一个低维子空间中的投影。

线性变换矩阵的形式取决于降维的目标。比如mds的目标是使得给定两个原样本点，它们之间的距离在降维前后保持不变，而pca的目标则是使得降维后最大程度在保留原样本包含的信息，同时样本点之间在新子空间中最大程度地分开。于是MDS和PCA算法最大的区别在于，前者对原样本集的距离矩阵做特征分解，而后者对原样本集的协方差矩阵做特征分解。MDS的作用是，在低维子空间中计算距离效率更高，对于原样本集来说信息量几乎没有改变。而pca的作用则是去掉原样本集中的一些冗余信息，使得模型更优。因此在现实中一般是PCA使用得比较多。

### PCA和特征分解
正如上文所说，PCA的关键在于对协方差矩阵做特征分解。在求得协方差矩阵的特征向量之后，用特征值最大的前$$k$$个特征向量对原样本集做线性变换，得到原样本集在低维空间上投影，在这个过程中舍去了一些只代表很小部分方差或信息量的变换方向。

首先从线性代数的角度来看，一个矩阵代表一个线性变换，线性变换的作用在于对原空间中的向量进行伸缩和旋转，旋转的形式由线性变换的方向决定。而线性变换同时也代表了一个新坐标系，因此"线性变换的方向"就对应于"新坐标系的坐标轴方向"，因此要确定一个线性变换的方向，相当于将它的坐标轴方向找出来。

另一方面，对一个坐标系来说，坐标轴上的单位向量具备的一个性质是，位于坐标系上的线性变换对坐标轴只起伸缩作用，不起旋转作用。以二维坐标系为例，对于坐标轴$$(0, 1)$$，坐标系上的一个线性变换$$((2, 0), (0, 3))$$, 这个线性变换作用在坐标轴$$(0, 1)$$上之后，效果是让$$(0, 1)$$拉伸成$$(0, 3) = 3 * (0, 1)$$, 相当于拉伸了3倍的长度。

综合以上两点可得，要找到一个坐标系的坐标轴方向，相当于找到那些"坐标系只产生伸缩效果，不产生旋转效果"的单位向量。于是这就是pca的理论基础$$Av  = \lambda v$$, 其中$$A$$代表要做pca的矩阵，$$v$$代表$$A$$对应的线性变换的坐标轴方向，即特征向量，$$\lambda$$代表$$A$$对各个特征向量的伸缩比例，对应于特征值。从中也可看出，某个坐标轴被这个线性变换作用之后，伸缩的比例越大，表示这个坐标轴越重要，也就对应于pca的性质之一：特征值大的特征向量更重要。

### PCA求解
