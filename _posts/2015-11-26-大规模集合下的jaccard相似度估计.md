---
layout: post
tags: 推荐算法
---

假设两个集合$$s_1$$和$$s_2$$，分别包含$$n_1$$、$$n_2$$个元素且元素之间没有特定顺序，并且这里的"集合"并不是准确的数学意义上的集合，元素可以有重复。
若要计算这两个集合的Jaccard相似度，分别计算$$s_1$$和$$s_2$$的交集、并集大小，当两个集合都比较大时，无论是时间成本还是空间成本都是比较高的。
最小哈希算法可以将两个大集合分别映射到两个较小的集合，用这两个小集合去估计原大集合的Jaccard相似度。算法主要包含两个部分：构造最小哈希序列；构造统计量$$r$$并进行估计。

### **构造最小哈希签名**
最小哈希函数$$h_{min}$$将一个集合映射到单个元素上。构造$$k$$个不同的$$h_{min}$$，就可以将一个集合映射到$$k$$个元素上，这$$k$$个元素组成的序列称为集合的最小哈希签名。

以$$n_1$$、$$n_2$$为例，构造步骤为：

1. 构造一个哈希函数h，将元素映射成实数值
2. 将h分别作用于$$n_1$$、$$n_2$$各自的元素，映射成两个新的实数序列$$h_1 = h_{min}(s_1)$$, $$h_2 = h_{min}(s_2)$$.
3. 将$$argmin_{e \in s_1} h_1$$作为$$s_1$$的最小哈希值，同样地将$$argmin_{e \in s_2} h_2$$作为$$s_2$$的最小哈希值

于是这就完成一个最小哈希签名的构造。从直观上来看，相当于每次将$$s_1$$和$$s_2$$包含的并集元素按不同的规则排序，从$$s_1$$和$$s_2$$抽元素时按照这个全局的顺序来抽。
这时候$$h_{min}(s_1) = argmin_{e \in s_1} h_1$$, $$h_{min}(s_2) = argmin_{e \in s_2} h_2$$, 可以看作是"用单个元素$$argmin_{e \in s_1} h_1$$代表集合$$s_1$$"。
可以看出哈希函数h并不是唯一的，只要构造$$k$$个哈希函数，就得到了$$k$$组最小哈希函数值，相当于可以用两个长度为$$k$$的序列代表原先的两个集合$$s_1$$和$$s_2$$，当$$k$$比$$n_1$$、$$n_2$$显著小时，就可以明显降低Jaccard相似度的计算量。

### **通过最小哈希签名来估计原集合的Jaccard相似度**
构造一个统计量$$r$$, 当$$h_{min}(s_1) == h_{min}(s_2)$$时，$$r$$取值为1，否则取值为0.
于是，$$r$$具备一个很好的性质：$$Prob(r=1) = Jaccard(s_1, s_2)$$. 直观上来看，当$$s_1$$和$$s_2$$包含的元素完全一致时，每一次构造新的哈希函数h，都可以得到$$h_{min}(s_1) == h_{min}(s_2)$$，即$$Prob(r=1) = 1$$, 而同时$$Jaccard(s_1, s_2) = 1$$。
有了这个性质之后，就可以通过估计$$Prob(r=1)$$来估计$$Jaccard(s_1, s_2)$$。而很明显$$r$$服从参数为$$Prob(r=1)$$二项分布，因此$$Prob(r=1)$$的估计值就是$$k$$个最小哈希值对里面，$$h_{min}(s_1) == h_{min}(s_2)$$的次数，即$$\frac{\sum_i^k I(h_{min,i}(s_1) == h_{min,i}(s_2))}{k}$$.

### **一些细节**

1. 适用于原集合较大的情况
最小哈希算法始终是**估计**真实的Jaccard相似度，所有的**估计**都有误差。理论上最小哈希算法估计的期望误差为$$O(\frac{1}{\sqrt k})$$, 于是$$k$$越大误差越小，而使用最小哈希的初衷是"压缩集合大小"，这就有一个权衡：$$k$$要尽可能大，但要比原集合长度显著小，否则没意义。
但当原集合"较大"时就不需要如此纠结，比如原集合长度在100万级别，那么$$k$$只需要取10000就可以让误差在$$0.01$$以内。

2. 集合元素有重并不会影响估计
在直接计算真实的Jaccard相似度时，需要将两个集合先去重，否则会使交集个数虚高。
但如果用最小哈希算法去估计时并不需要预先去重，因为它的一个重要步骤是"构造一个哈希函数h，将元素映射成实数值"，这表明值相同的两个元素的映射值是相等的，无论集合中有多少个共享这个值的元素，"这个值"被抽进哈希签名的概率是不会受影响的。

3. 需要构造"足够均匀"的哈希函数
即需要尽量避免将不同的元素哈希到相同的值去。试想一个极端的情况，构造的哈希函数将所有元素都映射成相同的值，最小哈希签名就相当于对原先两个集合分别进行$$k$$次随机抽样，这样每次抽取出来的"签名对"并不包含两个集合相似度的任何信息。
一般来说用质数足够大的乘法散列法就可以了。

### **小试验**
在以下两个词条集合上做试验：

> Statistical learning has emerged a emerged emerged emerged

> Statistical learning has aaaa a

真实的Jaccard相似度为0.67.

将原集合映射成长度为10的最小哈希签名：

{% highlight python %}
(('emerged', 'Statistical'),
('learning', 'learning'),
('a', 'aaaa'),
('emerged', 'has'),
('learning', 'learning'),
('has', 'has'),
('has', 'has'),
('learning', 'learning'),
('Statistical', 'Statistical'),
('Statistical', 'aaaa'))
{% endhighlight %}

可以看出10个签名对中元素值相同的有6对，于是Jaccard相似度估计值为0.6
当然这只是个试验，没有谁会把长度最大为8的集合映射成长度为10的签名。

### **C++代码块**

- **MinHashing.h**
{% highlight cpp %}
/*
 * File:   MinHashing.h
 * Author: wumengling
 *
 * Created on 2015年11月26日, 下午6:58
 */

#ifndef MINHASHING_H
#define MINHASHING_H

#include <map>
#include <string>
#include <array>
#include <random>
#include <fstream>
#include <sstream>
#include <vector>
#include <algorithm>
#include <set>
#include <functional>
#include <iostream>
#include <random>

typedef std::vector<std::string> doc;
typedef std::vector<doc> all_doc;
typedef std::set<std::string> doc_set;

typedef std::vector<std::string> signature;
typedef std::vector<signature> signature_matrix;



//构造哈希函数
int hash_func(std::string, int ,int);
//计算单个最小哈希值
std::string min_hashing(std::string, int, int);

//从文件中读入数据，并根据已构造的哈希函数和指定最小哈希序列长度，转成最小哈希签名储存
signature_matrix read_into_signature(std::string, int);

//用最小哈希签名估计Jaccard相似度
float jaccard_similarity(signature_matrix);

//从文件读数据入doc_vec
all_doc read(std::string);
float jaccard_similarity(doc, doc);
#endif /* MINHASHING_H */
{% endhighlight %}

- **MinHashing.cpp**
{% highlight cpp %}
#include "min_hashing.h"


//以两个文档为例，构造哈希函数

int hash_func(std::string word, int iter_time, int signature_size) {
    std::hash<std::string> str_hash;
    return ((iter_time * str_hash(word) + 1) % 10037) % signature_size;
}

//计算单个最小哈希值

std::string min_hashing(std::string one_doc, int iter_time, int signature_size) {
    std::string min_hashing_result = "";
    int smallest_val = signature_size;

    std::istringstream in_string(one_doc);
    std::string one_word;

    while (in_string >> one_word){
        int hash_val = hash_func(one_word, iter_time, signature_size);
        if (hash_val < smallest_val) {
            smallest_val = hash_val;
            min_hashing_result = one_word;
        }
    }
    return min_hashing_result;
}

//根据已构造的哈希函数和指定最小哈希序列长度，计算文档的最小哈希签名

signature_matrix read_into_signature(std::string file_path, int signature_size) {
    signature_matrix sig_mat;

    for (int i = 0; i < signature_size; i++) {
        signature signature_pair;
        std::ifstream in_file(file_path);
        std::string one_doc;

        while (std::getline(in_file, one_doc))
            signature_pair.push_back(min_hashing(one_doc, i + 1, signature_size));
        sig_mat.push_back(signature_pair);
    }
    return sig_mat;
}

//根据最小哈希签名估计Jaccard相似度

float jaccard_similarity(signature_matrix sig_mat) {
    float s = 0.0;
    for (auto sig_pair : sig_mat) {
        if (sig_pair[0] == sig_pair[1])
            s += 1.0;
    }
    return s / sig_mat.size();
}

//从文件读数据入doc_vec

all_doc read(std::string file_path) {
    all_doc all_docs;
    std::ifstream in_file(file_path);
    std::string line;
    while (std::getline(in_file, line)) {
        doc one_doc;
        std::istringstream in_string(line);
        std::string one_word;
        while (in_string >> one_word)
            one_doc.push_back(one_word);
        all_docs.push_back(one_doc);
    }
    return all_docs;
}

//计算文档对的Jaccard相似度

float jaccard_similarity(doc d1, doc d2) {
//需要先将两个集合去重，否则会出错
    float num_inter = 0.0;
    doc_set d1_set;
    doc_set d2_set;
    for (auto w: d1){
        d1_set.insert(w);
    }
    for (auto w : d2) {
        d2_set.insert(w);
    }
    for (auto each_word : d1_set) {
        if (d2_set.count(each_word) > 0)
            num_inter += 1.0;
    }
    return num_inter / (d1_set.size() + d2_set.size() - num_inter);
}
{% endhighlight %}

在推荐系统中经常要计算实体之间的相似度。

记实体数目为$$M$$, 特征维数为$$N$$, 实体对应的非零特征数目最大为$$L$$，实体特征矩阵为$$A_{M,N}$$。求实体两两之间的相似度最基础的步骤是要先求$$A^TA$$, 然后再根据不同的相似度度量做后续的处理。

在分布式环境下，假设有足够的机器能支持较大的并行度，那么计算$$A^TA$$的瓶颈主要在于shuffle size. 比如最简单粗暴的方法，将矩阵$$A$$按行分块存储于不同的节点，计算all-pairs相似度时需要获取存储在不同节点上的两行，这就带来$$O(n^2)$$数量级的shuffle，并且没有利用上相似度矩阵的对称性，相当于多了一倍的计算量。

当driver所在的节点能存储下矩阵$$A_{M,N}$$时，可以将矩阵$$A_{M,N}$$作为广播变量发送到各个节点，这样就能避免跨节点算相似度时带来的shuffle. 但问题在于一般真正用于生产的推荐系统的物品特征矩阵都不小，这也是使用分布式计算的初衷。

### Naive M/R算法
将物品特征矩阵$$A_{M,N}$$转置后分区存储，即每个分区上都有所有物品的部分特征数据。这样在做Map时只需要在分区内部进行计算：针对某分区中的一个元素$$r$$，对于元素中任意两个值$$r_i, r_j$$, 映射为元组$$(i,j), r_i * r_j$$。所有的shuffle都发生在Reduce阶段：

1. 先分别以$$i, j$$为key做按键值聚合，即聚合成$$(i, sum_j (r_i * r_j))$$、$$(j, sum_i (r_i * r_j))$$
2. 以$$(i,j)$$为key做按键值聚合，即聚合成$$((i, j), sum (r_i * r_j))$$
3. 由1、2的结果得到$$((i,j), \frac{sum (r_i * r_j)}{sum_j (r_i * r_j) * sum_i (r_i * r_j)})$$

3中得到的值对应的就是第$$i, j$$个物品对的cosine相似度。

### sampling方法（DISCO/DIMSUM v1/DIMSUM v2）
可以看出Naive M/R算法无论是Map还是Reduce阶段，计算量或shuffle size都与特征维度大小（即$$N$$）有关：

- 分区中共有$$N$$个需要做Map操作的元素$$r$$
- 以特定的$$(i^*, j^*)$$为键值的元组$$(i^*, j^*), r_{i^*} * r_{j^*}$$的数量量级在$$O(N)$$上

于是一种"dimension-independent"的相似度矩阵计算方法就被提出。这里所谓的dimension，在推荐系统的场景下一般是指实体的特征维度，比如基于内容推荐中的内容属性维度、itemCF中的用户数。它的思想是在Naive M/R算法的Map阶段进行重抽样，即

参考资料

- [Dimension Independent Similarity Computation](https://arxiv.org/pdf/1206.2082v4.pdf)
- [Dimension Independent Matrix Square using MapReduce](https://arxiv.org/pdf/1304.1467v4.pdf)
